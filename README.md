# Pipeline de Dados Financeiros | Databricks Data Lakehouse

## ðŸ“‹ Contexto do Projeto

Desenvolvimento de pipeline de dados financeiros para consolidaÃ§Ã£o e anÃ¡lise de informaÃ§Ãµes contÃ¡beis provenientes de mÃºltiplas fontes (Erathos e API REST), com foco em governanÃ§a, qualidade e performance.

## ðŸŽ¯ Desafio de NegÃ³cio

A organizaÃ§Ã£o necessitava de uma soluÃ§Ã£o robusta para:

- Centralizar dados financeiros dispersos em diferentes sistemas
- Garantir qualidade e consistÃªncia dos dados contÃ¡beis
- Prover acesso governado e auditÃ¡vel Ã s informaÃ§Ãµes
- Reduzir tempo de processamento e disponibilizaÃ§Ã£o de dados
- Facilitar anÃ¡lises de DFC, DRE, FCP e Saldos BancÃ¡rios

## ðŸ’¼ SoluÃ§Ã£o Implementada

### Arquitetura TÃ©cnica

Pipeline de dados baseado em **arquitetura medalhÃ£o** (Medallion Architecture) implementado na plataforma **Databricks**, utilizando **Delta Live Tables** para orquestraÃ§Ã£o declarativa e **Unity Catalog** para governanÃ§a centralizada.

### Camadas de Dados

#### **RAW Layer**
- IngestÃ£o de dados via API REST e plataforma Erathos
- Armazenamento em Unity Catalog Volumes (formato JSON)
- PreservaÃ§Ã£o de dados originais com metadados de rastreabilidade

#### **Bronze Layer**
- Streaming Tables para processamento contÃ­nuo e incremental
- PadronizaÃ§Ã£o de schemas e tipos de dados
- Particionamento por perÃ­odo (ano_mes) para otimizaÃ§Ã£o de queries
- Metadados de origem e timestamps de processamento

#### **Silver Layer**
- Live Tables com regras de negÃ³cio aplicadas
- Modelagem dimensional (Star Schema) com hierarquias de planos de contas
- DeduplicaÃ§Ã£o e normalizaÃ§Ã£o de dados
- Enriquecimento com tabelas dimensionais

#### **Gold Layer**
- Materialized Views para consumo analÃ­tico
- Dados consolidados e prontos para ferramentas de BI
- Performance otimizada para anÃ¡lises de negÃ³cio

## ðŸ› ï¸ Stack TecnolÃ³gico

**Plataforma:** Databricks Data Lakehouse  
**Storage:** Delta Lake (ACID transactions)  
**OrquestraÃ§Ã£o:** Delta Live Tables (DLT)  
**GovernanÃ§a:** Unity Catalog  
**Processamento:** Apache Spark (Spark SQL, Structured Streaming)  
**Linguagens:** SQL, PySpark  

## ðŸ“Š EntregÃ¡veis

### Tabelas Fato
- **fact_dfc** - DemonstraÃ§Ã£o de Fluxo de Caixa
- **fact_dre** - DemonstraÃ§Ã£o do Resultado do ExercÃ­cio
- **fact_fcp** - Fluxo de Contas a Pagar/Receber consolidado
- **fact_saldo** - PosiÃ§Ã£o diÃ¡ria de Saldos BancÃ¡rios

### Tabelas DimensÃ£o
- **dim_planos_dfc/dre/fcp** - Hierarquias de planos de contas (3 nÃ­veis)
- **dim_centro_custo_dre** - Centros de custo organizacionais
- **dim_normalizacao_descricao** - PadronizaÃ§Ã£o de descriÃ§Ãµes

### Componentes de IngestÃ£o
- Notebooks parametrizados para extraÃ§Ã£o via API REST
- IntegraÃ§Ã£o com plataforma Erathos
- Scripts de setup e manutenÃ§Ã£o (Unity Catalog, limpeza de volumes)

## ðŸ”§ Recursos TÃ©cnicos Implementados

### Performance
```sql
-- OtimizaÃ§Ã£o automÃ¡tica habilitada
'delta.autoOptimize.optimizeWrite' = 'true'
'delta.autoOptimize.autoCompact' = 'true'

-- Particionamento estratÃ©gico
PARTITIONED BY (ano_mes)
```

### GovernanÃ§a & Auditoria

- Unity Catalog com controle de acesso por catÃ¡logo/schema
- Change Data Feed (CDF) para rastreamento de alteraÃ§Ãµes
- Time Travel com retenÃ§Ã£o de 30 dias (logs) e 7 dias (arquivos)
- Metadados completos de origem e processamento

### Qualidade de Dados

- DeduplicaÃ§Ã£o via `QUALIFY ROW_NUMBER()` com ordenaÃ§Ã£o por timestamp
- NormalizaÃ§Ã£o de descriÃ§Ãµes atravÃ©s de tabela de mapeamento
- PadronizaÃ§Ã£o de campos texto (UPPER, TRIM)
- ConversÃ£o e validaÃ§Ã£o de tipos de dados

### Processamento Incremental

```sql
-- Streaming Tables para ingestÃ£o contÃ­nua
CREATE OR REFRESH STREAMING TABLE bronze.fact_saldo
FROM STREAM read_files('/Volumes/bitz/raw/saldo', format => 'json')
```

## ðŸ“ Arquitetura de Dados

```mermaid
graph TD
    A["FONTES DE DADOS - API REST | Erathos Platform"] --> B["RAW LAYER - Unity Catalog Volumes JSON"]
    B --> C["BRONZE LAYER - Streaming Tables DLT - PadronizaÃ§Ã£o | Particionamento"]
    C --> D["SILVER LAYER - Live Tables + Dimensions - Star Schema | DeduplicaÃ§Ã£o"]
    D --> E["GOLD LAYER - Materialized Views - Dados Consolidados para BI"]
    E --> F["CONSUMO - BI / Analytics"]
```

## ðŸ“‚ Estrutura do RepositÃ³rio

```
pipeline-bitz/
â”œâ”€â”€ ingestao/                    # MÃ³dulos de ingestÃ£o
â”‚   â”œâ”€â”€ get_saldo.ipynb           # ExtraÃ§Ã£o de saldos via API
â”‚   â”œâ”€â”€ get_pagar_receber.ipynb   # ExtraÃ§Ã£o contas pagar/receber
â”‚   â”œâ”€â”€ table_dfc_nivel_0.ipynb   # Processamento DFC (Erathos)
â”‚   â”œâ”€â”€ table_dre_nivel_0.ipynb   # Processamento DRE (Erathos)
â”‚   â”œâ”€â”€ table_fcp_nivel_0.ipynb   # Processamento FCP (Erathos)
â”‚   â””â”€â”€ table_dfc_intermediario.ipynb
â”‚
â”œâ”€â”€ pipeline/                    # Delta Live Tables (DLT)
â”‚   â”œâ”€â”€ bronze/                   # Camada Bronze
â”‚   â”‚   â”œâ”€â”€ fact_saldo.sql
â”‚   â”‚   â”œâ”€â”€ fact_contas_pagar_fcp.sql
â”‚   â”‚   â””â”€â”€ fact_contas_receber_fcp.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ silver/                   # Camada Silver
â”‚   â”‚   â”œâ”€â”€ fact_dfc.sql
â”‚   â”‚   â”œâ”€â”€ fact_dre.sql
â”‚   â”‚   â”œâ”€â”€ fact_saldo.sql
â”‚   â”‚   â”œâ”€â”€ fact_contas_pagar_fcp.sql
â”‚   â”‚   â”œâ”€â”€ fact_contas_receber_fcp.sql
â”‚   â”‚   â”œâ”€â”€ dim_planos_dfc.sql
â”‚   â”‚   â”œâ”€â”€ dim_planos_dre.sql
â”‚   â”‚   â””â”€â”€ dim_planos_fcp.sql
â”‚   â”‚
â”‚   â””â”€â”€ gold/                    # Camada Gold
â”‚       â”œâ”€â”€ fact_dfc.sql
â”‚       â”œâ”€â”€ fact_dre.sql
â”‚       â”œâ”€â”€ fact_fcp.sql
â”‚       â””â”€â”€ dim_centro_custo_dre.sql
â”‚
â””â”€â”€ utils/                    # Scripts utilitÃ¡rios
    â”œâ”€â”€ create_catalog_schema.py  # Setup Unity Catalog
    â”œâ”€â”€ normalizacao_descricao.ipynb
    â””â”€â”€ limpeza_volume.ipynb
```

## ðŸš€ ImplementaÃ§Ã£o

### 1. Setup Inicial

```python
# CriaÃ§Ã£o do catÃ¡logo e schemas no Unity Catalog
%run ./utils/create_catalog_schema.py
```

### 2. ConfiguraÃ§Ã£o do Pipeline DLT

- **Source Code:** DiretÃ³rio `pipeline/`
- **Target Catalog:** `bitz`
- **Pipeline Mode:** Triggered/Continuous
- **ConfiguraÃ§Ãµes:** Auto-optimize habilitado

### 3. ExecuÃ§Ã£o

1. Notebooks de ingestÃ£o (camada RAW)
2. Pipeline DLT (Bronze â†’ Silver â†’ Gold)
3. Scripts de manutenÃ§Ã£o (conforme necessÃ¡rio)

## ðŸ“ˆ Resultados e BenefÃ­cios

### TÃ©cnicos

- âœ… Pipeline totalmente automatizado e gerenciado via DLT
- âœ… Processamento incremental com Streaming Tables
- âœ… GovernanÃ§a centralizada com Unity Catalog
- âœ… Rastreabilidade completa (lineage) dos dados
- âœ… Qualidade garantida via deduplicaÃ§Ã£o e normalizaÃ§Ã£o

### NegÃ³cio

- âœ… ConsolidaÃ§Ã£o de dados financeiros de mÃºltiplas fontes
- âœ… ReduÃ§Ã£o significativa no tempo de disponibilizaÃ§Ã£o de dados
- âœ… Acesso governado e auditÃ¡vel Ã s informaÃ§Ãµes
- âœ… Base sÃ³lida para anÃ¡lises financeiras e tomada de decisÃ£o

## ðŸŽ“ CompetÃªncias TÃ©cnicas Demonstradas

**Data Engineering:**

- Arquitetura MedalhÃ£o (Medallion Architecture)
- Modelagem Dimensional (Star Schema)
- Processamento Incremental e Streaming
- ETL/ELT Patterns
- Data Quality & Governance

**Databricks Platform:**

- Delta Lake (ACID, Time Travel, CDF)
- Delta Live Tables (Declarative Pipelines)
- Unity Catalog (Governance)
- Structured Streaming
- Auto Loader

**Apache Spark:**

- Spark SQL (transformaÃ§Ãµes declarativas)
- PySpark (ingestÃ£o e processamento)
- OtimizaÃ§Ã£o de queries e particionamento

## ðŸ”— Tecnologias

`Databricks` `Delta Lake` `Delta Live Tables` `Unity Catalog` `Apache Spark` `PySpark` `Spark SQL` `Data Engineering` `ETL/ELT` `Data Lakehouse` `Medallion Architecture` `Streaming` `Data Governance` `Dimensional Modeling` `Star Schema`

---

**Projeto desenvolvido seguindo as melhores prÃ¡ticas de engenharia de dados moderna, com foco em escalabilidade, governanÃ§a e qualidade.**
